# -*- coding: utf-8 -*-
"""LitGen RAG Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y_whUuOmerE9RjRVBNeVCVQpAcLFnlo0
"""

!pip install pypdf
!pip install -U langchain langchain-community langchain-openai openai python-dotenv gradio chromadb PyMuPDF PyPDF2 scikit-learn plotly tiktoken

import os
import glob
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr
from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go
import fitz
from PyPDF2 import PdfReader
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
import re
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from typing import List, Dict, Any
from typing import List, Dict, Any, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from langchain.evaluation import load_evaluator
from langchain.evaluation.schema import StringEvaluator
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

MODEL = "gpt-4o-mini"
# gpt-4-turbo
db_name = "vector_db"

load_dotenv(override=True)
openai_api_key = os.getenv()

from google.colab import files
import os

# Upload PDF files (Knowledge Base)
uploaded = files.upload()

# Move PDFs to a dedicated folder
os.makedirs('pdfs', exist_ok=True)
for fname in uploaded:
    os.rename(fname, os.path.join('pdfs', fname))

# Find all PDFs in all topic folders
pdf_paths = glob.glob('/content/pdfs/**/*.pdf', recursive=True)
documents = []

for pdf_path in pdf_paths:
    # Extract folder name as doc_type (topic)
    doc_type = os.path.basename(os.path.dirname(pdf_path))
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    full_text = "\n".join([p.page_content for p in pages])
    doc = pages[0]  # Use first page's metadata
    doc.page_content = full_text
    doc.metadata["doc_type"] = doc_type
    doc.metadata["source_pdf"] = os.path.basename(pdf_path)
    documents.append(doc)

if not documents:
    raise ValueError("No documents found. Did you upload your PDFs?")

# Define headers to split on
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
    ("####", "Header 4"),
]

def semantic_splitter(text):
    """
    Split text into semantic chunks based on headers and natural breaks
    """
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    header_splits = markdown_splitter.split_text(text)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=250,
        separators=["\n\n", "\n", ". ", " ", ""],
        length_function=len,
    )

    final_chunks = []
    for split in header_splits:
        metadata = split.metadata.copy()
        if header_splits:
            metadata["section"] = split.metadata.get("Header 1", "") + " " + split.metadata.get("Header 2", "")

        # Split the content
        sub_chunks = text_splitter.split_text(split.page_content)
        for chunk in sub_chunks:
            final_chunks.append(Document(
                page_content=chunk,
                metadata=metadata
            ))

    return final_chunks

chunks = []
for doc in documents:
    semantic_chunks = semantic_splitter(doc.page_content)
    for chunk in semantic_chunks:
        # Preserve original metadata
        chunk.metadata.update(doc.metadata)
    chunks.extend(semantic_chunks)

if chunks:
    print(f"Split into {len(chunks)} semantic chunks")
    print(f"Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} chars")
else:
    print("No chunks created. Check that your documents are loaded correctly.")

doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"Document types found: {', '.join(doc_types)}")

# Embed & Build Vectorstore
import shutil
import os
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

db_name = "/content/vector_db"

if os.path.exists(db_name):
    shutil.rmtree(db_name)
    print(f"Deleted existing Chroma DB directory: {db_name}")

openai_api_key = ''


embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

# Create the vectorstore from the chunked documents
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory=db_name,
    collection_metadata={"hnsw:space": "cosine"}
)

# Safely access internal collection for stats
collection = vectorstore._collection
num_documents = collection.count()
print(f"Vectorstore created with {num_documents} documents")

# Retrieve vectors, texts, and metadatas
result = collection.get(include=['embeddings', 'documents', 'metadatas'])
vectors = np.array(result['embeddings'])
retrieved_texts = result['documents']
metadatas = result['metadatas']

doc_types = [metadata.get('doc_type', 'Unknown') for metadata in metadatas]

# Color mapping for visualization
color_map = {
    'Deep Learning': 'red',
    'Generative AI': 'blue',
    'Large Language Models': 'green'
}
colors = [color_map.get(t, 'gray') for t in doc_types]

print("Number of vectors:", len(vectors))
if len(vectors) > 0:
    dimensions = len(vectors[0])
    print(f"The vectors have {dimensions:,} dimensions")
else:
    print("No vectors found.")

# Vector Store Visualization
from sklearn.manifold import TSNE
import plotly.graph_objects as go
import numpy as np

if len(vectors) > 1:  # t-SNE requires at least 2 samples
    tsne = TSNE(n_components=2, perplexity=2, random_state=42)
    reduced_vectors = tsne.fit_transform(vectors)
else:
    reduced_vectors = np.zeros((len(vectors), 2))

fig = go.Figure(data=[go.Scatter(
    x=reduced_vectors[:, 0],
    y=reduced_vectors[:, 1],
    mode='markers',
    marker=dict(size=5, color=colors, opacity=0.8),
    text=[f"Type: {t}<br>Text: {txt[:100]}..." for t, txt in zip(doc_types, retrieved_texts)],
    hoverinfo='text'
)])

fig.update_layout(
    title='2D Chroma Vector Store Visualization',
    xaxis_title='x',
    yaxis_title='y',
    width=600,
    height=400,
    margin=dict(r=20, b=10, l=10, t=40)
)

fig.show()

###RETRIEVAL & GENERATION
os.environ["OPENAI_API_KEY"] = ""

# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# demonstrate hybrid search
def hybrid_search(query, k=3):
    """
    Perform hybrid search combining keyword and vector search
    """
    # Get results from both search methods
    vector_results = vectorstore.similarity_search(query, k=k)
    keyword_results = vectorstore.similarity_search_with_score(query, k=k, filter=None)

    # Combine and deduplicate results
    seen_docs = set()
    combined_results = []

    # Add vector results first
    for doc in vector_results:
        if doc.page_content not in seen_docs:
            combined_results.append(doc)
            seen_docs.add(doc.page_content)

    # Add keyword results
    for doc, score in keyword_results:
        if doc.page_content not in seen_docs:
            combined_results.append(doc)
            seen_docs.add(doc.page_content)

    return combined_results[:k]

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

class EnhancedConversationalRetrievalChain:
    def __init__(self, llm, retriever, memory):
        self.llm = llm
        self.retriever = retriever
        self.memory = memory

        # Query reformulation prompt
        self.query_reformulation_prompt = PromptTemplate(
            input_variables=["question", "chat_history"],
            template="""Given the following question and chat history, reformulate the question to better capture the user's intent.
            Consider:
            1. Any implicit context from the chat history
            2. Potential ambiguities in the original question
            3. Key terms that should be emphasized

            Chat history: {chat_history}
            Original question: {question}

            Provide 2-3 reformulated versions of the question, each focusing on different aspects:"""
        )

        # Key facts extraction prompt
        self.facts_extraction_prompt = PromptTemplate(
            input_variables=["context"],
            template="""Extract key facts from the following context that are relevant to answering the question.
            For each fact:
            1. Quote the exact text from the context
            2. Note the source (document/section)
            3. Rate its relevance (1-5)

            Context: {context}

            Extract and format the key facts:"""
        )

        # Modified step-back prompt to focus on context requirements
        self.step_back_prompt = PromptTemplate(
            input_variables=["question", "reformulated_queries", "extracted_facts"],
            template="""Given the following question and reformulated versions, identify what specific information from the context would be needed to answer it properly.
            Use the extracted facts to guide your analysis.

            Original question: {question}
            Reformulated queries: {reformulated_queries}
            Extracted facts: {extracted_facts}

            What specific information from the context is required to answer this question? List the key facts needed."""
        )

        # Enhanced answer prompt with strict context grounding and citation
        self.answer_prompt = PromptTemplate(
            input_variables=["context", "question", "chat_history", "required_info", "extracted_facts"],
            template="""You are a helpful AI assistant. Your task is to answer the question using ONLY the provided context.
            You MUST cite specific parts of the context that support your answer.

            Required information from context:
            {required_info}

            Extracted key facts:
            {extracted_facts}

            Context: {context}

            Chat history: {chat_history}

            Question: {question}

            Instructions:
            1. Only use information explicitly stated in the context
            2. Cite specific parts of the context using [Source: section/page]
            3. If you need to make an inference, clearly state it's an inference
            4. If the context doesn't provide enough information, say so
            5. Structure your answer with clear sections and bullet points where appropriate

            Answer:"""
        )

        # Modified refinement prompt to enforce context grounding and alignment
        self.refinement_prompt = PromptTemplate(
            input_variables=["context", "question", "initial_answer", "required_info", "extracted_facts"],
            template="""Review the following answer and ensure it strictly uses the provided context.

            Required information from context:
            {required_info}

            Extracted key facts:
            {extracted_facts}

            Context: {context}
            Question: {question}
            Initial answer: {initial_answer}

            Instructions:
            1. Remove any information not directly supported by the context
            2. Ensure all claims are properly cited
            3. Check alignment between the answer and the question's intent
            4. Verify that all key facts are accurately represented
            5. Clearly mark any inferences
            6. If information is missing, acknowledge the gaps

            Provide an improved version of the answer:"""
        )

        # Initialize chains
        self.query_reformulation_chain = LLMChain(llm=llm, prompt=self.query_reformulation_prompt)
        self.facts_extraction_chain = LLMChain(llm=llm, prompt=self.facts_extraction_prompt)
        self.step_back_chain = LLMChain(llm=llm, prompt=self.step_back_prompt)
        self.answer_chain = LLMChain(llm=llm, prompt=self.answer_prompt)
        self.refinement_chain = LLMChain(llm=llm, prompt=self.refinement_prompt)

    def invoke(self, question: str) -> Dict[str, Any]:
        # Step 1: Query reformulation
        chat_history = self.memory.chat_memory.messages
        reformulation = self.query_reformulation_chain.invoke({
            "question": question,
            "chat_history": chat_history
        })

        # Step 2: Retrieve relevant documents with increased k for better coverage
        docs = self.retriever.get_relevant_documents(question, k=5)
        context = "\n\n".join([doc.page_content for doc in docs])

        # Step 3: Extract key facts
        facts = self.facts_extraction_chain.invoke({"context": context})

        # Step 4: Take a step back and identify required context
        step_back_analysis = self.step_back_chain.invoke({
            "question": question,
            "reformulated_queries": reformulation["text"],
            "extracted_facts": facts["text"]
        })
        required_info = step_back_analysis["text"]

        # Step 5: Generate initial answer with strict context requirements
        initial_answer = self.answer_chain.invoke({
            "context": context,
            "question": question,
            "chat_history": chat_history,
            "required_info": required_info,
            "extracted_facts": facts["text"]
        })

        # Step 6: Self-refinement with context verification and alignment check
        refined_answer = self.refinement_chain.invoke({
            "context": context,
            "question": question,
            "initial_answer": initial_answer["text"],
            "required_info": required_info,
            "extracted_facts": facts["text"]
        })

        # Step 7: Calculate confidence based on context coverage and alignment
        context_coverage = self._calculate_context_coverage(refined_answer["text"], context)
        alignment_score = self._calculate_alignment_score(refined_answer["text"], question)
        evidence_quality = self._calculate_evidence_quality(refined_answer["text"])
        confidence_score = self._calculate_confidence_score(
            refined_answer["text"],
            context,
            question
        )

        # Update memory
        self.memory.chat_memory.add_user_message(question)
        self.memory.chat_memory.add_ai_message(refined_answer["text"])

        return {
            "answer": refined_answer["text"],
            "reformulated_queries": reformulation["text"],
            "extracted_facts": facts["text"],
            "step_back_analysis": step_back_analysis["text"],
            "initial_answer": initial_answer["text"],
            "confidence": {
                "score": confidence_score,
                "context_coverage": context_coverage,
                "alignment_score": alignment_score,
                "evidence_quality": evidence_quality,
                "required_info": required_info
            }
        }

    def _calculate_context_coverage(self, answer: str, context: str) -> float:
        """Calculate how much of the answer is grounded in the context"""
        # Enhanced implementation with citation checking and semantic analysis
        answer_sentences = [s.strip() for s in answer.split('.') if s.strip()]
        context_sentences = [s.strip() for s in context.split('.') if s.strip()]

        if not answer_sentences:
            return 0.0

        grounded_scores = []
        for ans_sent in answer_sentences:
            # Check for citations
            citations = re.findall(r'\[Source:.*?\]', ans_sent)
            if citations:
                grounded_scores.append(1.0)  # Full score for cited sentences
                continue

            # Check for semantic grounding
            max_similarity = 0.0
            for ctx_sent in context_sentences:
                similarity = self._calculate_sentence_similarity(ans_sent, ctx_sent)
                max_similarity = max(max_similarity, similarity)

            # Consider a sentence grounded if similarity > 0.6
            grounded_scores.append(1.0 if max_similarity > 0.6 else max_similarity)

        return sum(grounded_scores) / len(grounded_scores)

    def _calculate_alignment_score(self, answer: str, question: str) -> float:
        """Calculate how well the answer aligns with the question's intent"""
        # Enhanced implementation with semantic similarity and key phrase matching
        question_keywords = set(re.findall(r'\w+', question.lower()))
        answer_keywords = set(re.findall(r'\w+', answer.lower()))

        # Calculate keyword overlap
        keyword_overlap = len(question_keywords.intersection(answer_keywords)) / len(question_keywords) if question_keywords else 0

        # Check for question-specific elements
        has_citations = bool(re.search(r'\[Source:', answer))
        has_inferences = bool(re.search(r'\[Inference:', answer))
        has_limitations = bool(re.search(r'\[Limitation:', answer))

        # Calculate semantic similarity between question and answer
        semantic_similarity = self._calculate_sentence_similarity(question, answer)

        # Enhanced scoring with adjusted weights
        alignment_score = (
            keyword_overlap * 0.3 +
            semantic_similarity * 0.3 +
            has_citations * 0.2 +
            has_inferences * 0.1 +
            has_limitations * 0.1
        )

        return alignment_score

    def _calculate_sentence_similarity(self, sent1: str, sent2: str) -> float:
        """Calculate semantic similarity between two sentences"""
        # Simple implementation using word overlap and length ratio
        words1 = set(sent1.lower().split())
        words2 = set(sent2.lower().split())

        # Calculate Jaccard similarity
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        jaccard = intersection / union if union > 0 else 0

        # Consider length ratio
        len_ratio = min(len(sent1), len(sent2)) / max(len(sent1), len(sent2))

        # Combine scores
        return 0.7 * jaccard + 0.3 * len_ratio

    def _calculate_evidence_quality(self, answer: str) -> float:
        """Calculate the quality of evidence in the answer"""
        # Check for citations
        citations = re.findall(r'\[Source:.*?\]', answer)
        num_citations = len(citations)

        # Check for specific evidence markers
        has_quotes = bool(re.search(r'["\'].*?["\']', answer))
        has_numbers = bool(re.search(r'\d+', answer))
        has_specific_terms = bool(re.search(r'(specifically|particularly|notably|especially)', answer.lower()))

        # Calculate evidence score
        evidence_score = (
            min(num_citations / 3, 1.0) * 0.4 +  # Cap at 3 citations
            has_quotes * 0.2 +
            has_numbers * 0.2 +
            has_specific_terms * 0.2
        )

        return evidence_score

    def _calculate_confidence_score(self, answer: str, context: str, question: str) -> int:
        """Calculate a comprehensive confidence score"""
        # Calculate individual metrics
        context_coverage = self._calculate_context_coverage(answer, context)
        alignment_score = self._calculate_alignment_score(answer, question)
        evidence_quality = self._calculate_evidence_quality(answer)

        # Calculate final confidence score with adjusted weights
        confidence_score = (
            context_coverage * 0.4 +      # How well grounded in context
            alignment_score * 0.3 +       # How well it answers the question
            evidence_quality * 0.3        # Quality of supporting evidence
        )

        # Scale to 0-100 and round
        return min(100, int(confidence_score * 100))

# Replace the existing conversation chain with our enhanced version
conversation_chain = EnhancedConversationalRetrievalChain(
    llm=llm,
    retriever=retriever,
    memory=memory
)

# Update the chat function to use the enhanced chain
def chat(message, history):
    result = conversation_chain.invoke(message)
    confidence_info = f"\n\nConfidence: {result['confidence']['score']}/100\n{result['confidence']['required_info']}"
    return result["answer"] + confidence_info

view = gr.ChatInterface(chat).launch(share=True)

### EVALUATION

class KnowledgeBaseVerifier:
    def __init__(self, llm):
        self.llm = llm

        # Prompt to identify information sources
        self.source_verification_prompt = PromptTemplate(
            input_variables=["answer", "context", "question"],
            template="""Analyze the following answer and determine which parts come from the provided context versus the model's pre-trained knowledge.

Question: {question}
Answer: {answer}
Context: {context}

For each key piece of information in the answer, identify its source:
1. If it comes from the context, mark it as [FROM_CONTEXT]
2. If it's likely from the model's pre-trained knowledge, mark it as [FROM_KNOWLEDGE]
3. If it's an inference or combination, mark it as [INFERRED]

Provide your analysis in this format:
- Information 1: [SOURCE] (explanation)
- Information 2: [SOURCE] (explanation)
..."""
        )

        self.verification_chain = LLMChain(llm=llm, prompt=self.source_verification_prompt)

    def verify_answer_sources(self, answer: str, context: str, question: str) -> Dict[str, Any]:
        """Verify the sources of information in the answer"""
        verification = self.verification_chain.invoke({
            "answer": answer,
            "context": context,
            "question": question
        })

        # Parse the verification results
        info_points = []
        context_based = 0
        knowledge_based = 0
        inferred = 0

        for line in verification["text"].split("\n"):
            if line.strip().startswith("-"):
                match = re.search(r"\[(.*?)\]", line)
                if match:
                    source = match.group(1)
                    if source == "FROM_CONTEXT":
                        context_based += 1
                    elif source == "FROM_KNOWLEDGE":
                        knowledge_based += 1
                    elif source == "INFERRED":
                        inferred += 1
                    info_points.append(line.strip())

        total_points = context_based + knowledge_based + inferred
        if total_points == 0:
            return {
                "context_ratio": 0,
                "knowledge_ratio": 0,
                "inferred_ratio": 0,
                "verification_details": verification["text"],
                "info_points": info_points
            }

        return {
            "context_ratio": context_based / total_points,
            "knowledge_ratio": knowledge_based / total_points,
            "inferred_ratio": inferred / total_points,
            "verification_details": verification["text"],
            "info_points": info_points
        }

def simple_evaluate(query: str, conversation_chain, verifier: KnowledgeBaseVerifier = None) -> dict:
    """Simple evaluation of a single query with knowledge base verification"""
    try:
        result = conversation_chain.invoke(query)
        docs = conversation_chain.retriever.get_relevant_documents(query)
        context = "\n\n".join([doc.page_content for doc in docs])

        evaluation = {
            "query": query,
            "answer": result["answer"],
            "confidence": result["confidence"]["score"],
            "retrieved_docs": len(docs)
        }

        # Add knowledge base verification if verifier is provided
        if verifier:
            verification = verifier.verify_answer_sources(
                result["answer"],
                context,
                query
            )
            evaluation["verification"] = verification

        return evaluation
    except Exception as e:
        return {
            "query": query,
            "error": str(e)
        }

def run_diverse_evaluation(conversation_chain):
    """Run evaluation with diverse query types and knowledge base verification"""

    # Initialize the knowledge base verifier
    verifier = KnowledgeBaseVerifier(llm)

    # Define test queries by category with passage-specific questions
    test_categories = {
        "✅ Easy (Factual / Recall-Based)": [
            "What are the model size parameters for BERTBASE and BERTLARGE?",
            "What two corpora were used for pre-training BERT?",
            "What is the purpose of the [CLS] token in BERT's input?",
            "What percentage of tokens are masked during pre-training in BERT?",
            "What embedding types are summed to form BERT's input representations?"
        ],
        "📘 Domain-Specific (Technical / Paper-Specific)": [
            "How does BERT use self-attention to unify cross-attention across sentence pairs?",
            "Explain how BERT mitigates the pre-training and fine-tuning mismatch introduced by the [MASK] token.",
            "What is the difference between bidirectional and constrained self-attention in BERT vs. GPT?",
            "Why is a document-level corpus important for BERT's pre-training?",
            "How does the Next Sentence Prediction (NSP) task benefit question answering and NLI?"
        ],
        "❓ Ambiguous / Reasoning-Based": [
            "Could BERT function effectively without the NSP task?",
            "In what situations might the [CLS] token fail to represent a sentence effectively?",
            "Does masking 15% of tokens negatively affect the downstream performance of BERT? Why or why not?",
            "How would BERT perform if trained on a shuffled corpus like the Billion Word Benchmark?",
            "What limitations might arise from only using the [MASK] token 80% of the time during MLM?"
        ]
    }

    print("\n=== RAG System Evaluation ===\n")
    print("Testing system with diverse query types to assess performance across different complexity levels")
    print("Including knowledge base verification to ensure answers are based on provided documents\n")

    # Track overall statistics
    overall_stats = {
        "total_queries": 0,
        "total_confidence": 0,
        "total_docs": 0,
        "total_context_ratio": 0,
        "total_knowledge_ratio": 0,
        "total_inferred_ratio": 0,
        "category_stats": {}
    }

    # Evaluate each category
    for category, queries in test_categories.items():
        print(f"\n{category}:")
        print("-" * 80)

        category_results = []
        for query in queries:
            result = simple_evaluate(query, conversation_chain, verifier)
            category_results.append(result)

            # Print results with enhanced formatting
            print(f"\nQuery: {result['query']}")
            if "error" in result:
                print(f"Error: {result['error']}")
            else:
                print(f"Answer: {result['answer']}")
                print(f"Confidence: {result['confidence']}/100")
                print(f"Retrieved Documents: {result['retrieved_docs']}")

                if "verification" in result:
                    v = result["verification"]
                    print("\nKnowledge Base Usage:")
                    print(f"Context-based information: {v['context_ratio']*100:.1f}%")
                    print(f"Pre-trained knowledge: {v['knowledge_ratio']*100:.1f}%")
                    print(f"Inferred information: {v['inferred_ratio']*100:.1f}%")

                    # Print detailed analysis with better formatting
                    print("\nDetailed Analysis:")
                    for point in v["info_points"]:
                        # Extract source and explanation
                        source_match = re.search(r"\[(.*?)\]", point)
                        if source_match:
                            source = source_match.group(1)
                            explanation = point.split("]")[-1].strip()
                            print(f"  • [{source}] {explanation}")

                # Update overall statistics
                overall_stats["total_queries"] += 1
                overall_stats["total_confidence"] += result["confidence"]
                overall_stats["total_docs"] += result["retrieved_docs"]
                if "verification" in result:
                    v = result["verification"]
                    overall_stats["total_context_ratio"] += v["context_ratio"]
                    overall_stats["total_knowledge_ratio"] += v["knowledge_ratio"]
                    overall_stats["total_inferred_ratio"] += v["inferred_ratio"]

        # Calculate and print category statistics with enhanced formatting
        if category_results and "error" not in category_results[0]:
            avg_confidence = sum(r["confidence"] for r in category_results) / len(category_results)
            avg_docs = sum(r["retrieved_docs"] for r in category_results) / len(category_results)

            # Calculate knowledge base usage statistics
            context_ratios = [r.get("verification", {}).get("context_ratio", 0) for r in category_results]
            knowledge_ratios = [r.get("verification", {}).get("knowledge_ratio", 0) for r in category_results]
            inferred_ratios = [r.get("verification", {}).get("inferred_ratio", 0) for r in category_results]

            print(f"\nCategory Statistics:")
            print(f"• Average Confidence: {avg_confidence:.1f}/100")
            print(f"• Average Retrieved Documents: {avg_docs:.1f}")
            print(f"• Knowledge Base Usage:")
            print(f"  - Context-based: {np.mean(context_ratios)*100:.1f}%")
            print(f"  - Pre-trained: {np.mean(knowledge_ratios)*100:.1f}%")
            print(f"  - Inferred: {np.mean(inferred_ratios)*100:.1f}%")

            # Store category stats
            overall_stats["category_stats"][category] = {
                "avg_confidence": avg_confidence,
                "avg_docs": avg_docs,
                "num_queries": len(category_results),
                "context_ratio": np.mean(context_ratios),
                "knowledge_ratio": np.mean(knowledge_ratios),
                "inferred_ratio": np.mean(inferred_ratios)
            }

        print("-" * 80)

    # Print overall statistics with enhanced formatting
    if overall_stats["total_queries"] > 0:
        print("\n=== Overall Statistics ===")
        print(f"Total Queries Evaluated: {overall_stats['total_queries']}")
        print(f"Overall Average Confidence: {overall_stats['total_confidence']/overall_stats['total_queries']:.1f}/100")
        print(f"Overall Average Retrieved Documents: {overall_stats['total_docs']/overall_stats['total_queries']:.1f}")

        print("\nOverall Knowledge Base Usage:")
        print(f"• Context-based information: {overall_stats['total_context_ratio']/overall_stats['total_queries']*100:.1f}%")
        print(f"• Pre-trained knowledge: {overall_stats['total_knowledge_ratio']/overall_stats['total_queries']*100:.1f}%")
        print(f"• Inferred information: {overall_stats['total_inferred_ratio']/overall_stats['total_queries']*100:.1f}%")

        print("\nPerformance by Category:")
        for category, stats in overall_stats["category_stats"].items():
            print(f"\n{category}:")
            print(f"  • Average Confidence: {stats['avg_confidence']:.1f}/100")
            print(f"  • Average Retrieved Documents: {stats['avg_docs']:.1f}")
            print(f"  • Knowledge Base Usage:")
            print(f"    - Context-based: {stats['context_ratio']*100:.1f}%")
            print(f"    - Pre-trained: {stats['knowledge_ratio']*100:.1f}%")
            print(f"    - Inferred: {stats['inferred_ratio']*100:.1f}%")
            print(f"  • Number of Queries: {stats['num_queries']}")

# Function for running evaluations when needed
def run_evaluations(evaluation_type="all"):
    """
    Run evaluations based on the specified type.

    Args:
        evaluation_type (str): Type of evaluation to run. Options:
            - "all": Run both diverse and literature review evaluations
            - "diverse": Run only the diverse evaluation
            - "literature": Run only the literature review evaluation
    """
    if evaluation_type in ["all", "diverse"]:
        print("Starting diverse evaluation...")
        run_diverse_evaluation(conversation_chain)

    if evaluation_type in ["all", "literature"]:
        print("\n" + "="*80 + "\n")
        print("Starting literature review evaluation...")
        run_literature_review_evaluation(conversation_chain)

# The evaluations will now only run when explicitly called using:
# run_evaluations()  # Run all evaluations
# run_evaluations("diverse")  # Run only diverse evaluation
# run_evaluations("literature")  # Run only literature review evaluation

run_evaluations("diverse")

def run_literature_review_evaluation(conversation_chain):
    """Run evaluation with literature review and research-focused queries"""

    # Initialize the knowledge base verifier
    verifier = KnowledgeBaseVerifier(llm)

    # Define test categories for literature review
    test_categories = {
        " Literature Review & Summarization": [
            {
                "query": "Summarize the key architectural distinctions between encoder-decoder and decoder-only models as discussed in 'Fundamentals of Generative Large Language Models'.",
                "goal": "Test how well the model grounds architecture comparisons in context."
            },
            {
                "query": "What are the primary categories of generative AI applications discussed in 'A Survey of Generative AI Applications' and how do they differ across industries?",
                "goal": "Retrieve and consolidate category-specific use cases from a structured survey."
            }
        ],
        " Paper Comparison & Synthesis": [
            {
                "query": "Compare how BERT and GPT are pre-trained and fine-tuned, using both the BERT paper and the Fundamentals paper.",
                "goal": "Assess multi-document synthesis and context-citation integration."
            },
            {
                "query": "How does instruction tuning differ from supervised fine-tuning, and which models used which?",
                "goal": "Validate nuanced understanding of fine-tuning strategies across papers."
            }
        ],
        " Systematic Review Support": [
            {
                "query": "What techniques are used to evaluate generative models beyond BLEU and ROUGE?",
                "goal": "Retrieve domain-specific evaluation techniques to aid research reproducibility."
            },
            {
                "query": "According to the literature, what are the limitations of current benchmarking datasets for generative models?",
                "goal": "Extract cited gaps for new research opportunities."
            }
        ],
        " Citation & Argumentation": [
            {
                "query": "Provide arguments in favor of prompt engineering as a vital part of model performance, with citations.",
                "goal": "Generate source-supported claims, testing traceability."
            },
            {
                "query": "Is the claim that generative models are 'emergent learners' supported in the Fundamentals paper?",
                "goal": "Evaluate model's ability to challenge or verify claims against academic context."
            }
        ],
        " Educational Support": [
            {
                "query": "Why is masked language modeling considered more effective for bidirectional encoding in BERT?",
                "goal": "Offer pedagogical clarity for complex foundational concepts with citations."
            }
        ]
    }

    print("\n=== Literature Review Evaluation ===\n")
    print("Testing system with research-focused queries to assess academic literature handling capabilities\n")

    # Track overall statistics
    overall_stats = {
        "total_queries": 0,
        "total_confidence": 0,
        "total_docs": 0,
        "total_context_ratio": 0,
        "total_knowledge_ratio": 0,
        "total_inferred_ratio": 0,
        "category_stats": {}
    }

    # Evaluate each category
    for category, queries in test_categories.items():
        print(f"\n{category}:")
        print("-" * 80)

        category_results = []
        for query_info in queries:
            query = query_info["query"]
            goal = query_info["goal"]

            print(f"\nQuery: {query}")
            print(f"Goal: {goal}")

            result = simple_evaluate(query, conversation_chain, verifier)
            category_results.append(result)

            if "error" in result:
                print(f"Error: {result['error']}")
            else:
                print(f"\nAnswer: {result['answer']}")
                print(f"Confidence: {result['confidence']}/100")
                print(f"Retrieved Documents: {result['retrieved_docs']}")

                if "verification" in result:
                    v = result["verification"]
                    print("\nKnowledge Base Usage:")
                    print(f"• Context-based: {v['context_ratio']*100:.1f}%")
                    print(f"• Pre-trained: {v['knowledge_ratio']*100:.1f}%")
                    print(f"• Inferred: {v['inferred_ratio']*100:.1f}%")

                    # Print detailed analysis with better formatting
                    print("\nDetailed Analysis:")
                    for point in v["info_points"]:
                        source_match = re.search(r"\[(.*?)\]", point)
                        if source_match:
                            source = source_match.group(1)
                            explanation = point.split("]")[-1].strip()
                            print(f"  • [{source}] {explanation}")

                # Update overall statistics
                overall_stats["total_queries"] += 1
                overall_stats["total_confidence"] += result["confidence"]
                overall_stats["total_docs"] += result["retrieved_docs"]
                if "verification" in result:
                    v = result["verification"]
                    overall_stats["total_context_ratio"] += v["context_ratio"]
                    overall_stats["total_knowledge_ratio"] += v["knowledge_ratio"]
                    overall_stats["total_inferred_ratio"] += v["inferred_ratio"]

        # Calculate and print category statistics
        if category_results and "error" not in category_results[0]:
            avg_confidence = sum(r["confidence"] for r in category_results) / len(category_results)
            avg_docs = sum(r["retrieved_docs"] for r in category_results) / len(category_results)

            # Calculate knowledge base usage statistics for the category
            context_ratios = [r.get("verification", {}).get("context_ratio", 0) for r in category_results]
            knowledge_ratios = [r.get("verification", {}).get("knowledge_ratio", 0) for r in category_results]
            inferred_ratios = [r.get("verification", {}).get("inferred_ratio", 0) for r in category_results]

            print(f"\nCategory Statistics:")
            print(f"• Average Confidence: {avg_confidence:.1f}/100")
            print(f"• Average Retrieved Documents: {avg_docs:.1f}")
            print(f"• Knowledge Base Usage:")
            print(f"  - Context-based: {np.mean(context_ratios)*100:.1f}%")
            print(f"  - Pre-trained: {np.mean(knowledge_ratios)*100:.1f}%")
            print(f"  - Inferred: {np.mean(inferred_ratios)*100:.1f}%")

            # Store category stats
            overall_stats["category_stats"][category] = {
                "avg_confidence": avg_confidence,
                "avg_docs": avg_docs,
                "num_queries": len(category_results),
                "context_ratio": np.mean(context_ratios),
                "knowledge_ratio": np.mean(knowledge_ratios),
                "inferred_ratio": np.mean(inferred_ratios)
            }

        print("-" * 80)

    # Print overall statistics
    if overall_stats["total_queries"] > 0:
        print("\n=== Overall Statistics ===")
        print(f"Total Queries Evaluated: {overall_stats['total_queries']}")
        print(f"Overall Average Confidence: {overall_stats['total_confidence']/overall_stats['total_queries']:.1f}/100")
        print(f"Overall Average Retrieved Documents: {overall_stats['total_docs']/overall_stats['total_queries']:.1f}")

        print("\nOverall Knowledge Base Usage:")
        print(f"• Context-based information: {overall_stats['total_context_ratio']/overall_stats['total_queries']*100:.1f}%")
        print(f"• Pre-trained knowledge: {overall_stats['total_knowledge_ratio']/overall_stats['total_queries']*100:.1f}%")
        print(f"• Inferred information: {overall_stats['total_inferred_ratio']/overall_stats['total_queries']*100:.1f}%")

        print("\nPerformance by Category:")
        for category, stats in overall_stats["category_stats"].items():
            print(f"\n{category}:")
            print(f"  • Average Confidence: {stats['avg_confidence']:.1f}/100")
            print(f"  • Average Retrieved Documents: {stats['avg_docs']:.1f}")
            print(f"  • Knowledge Base Usage:")
            print(f"    - Context-based: {stats['context_ratio']*100:.1f}%")
            print(f"    - Pre-trained: {stats['knowledge_ratio']*100:.1f}%")
            print(f"    - Inferred: {stats['inferred_ratio']*100:.1f}%")
            print(f"  • Number of Queries: {stats['num_queries']}")

run_evaluations("literature")